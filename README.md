# Torch2TRT Example

This repository demonstrates how to convert a PyTorch model to TensorRT using the `torch2trt` library and perform inference with the converted model.

## Requirements

1. **Python 3.6+**
2. **PyTorch** - Make sure you have PyTorch installed.
3. **TensorRT** - You will need TensorRT to optimize the model for GPU inference.
4. **CUDA** - Ensure you have CUDA installed for GPU support.

You can install the necessary dependencies by following the instructions below.

## Setup

### 1. Clone the repository:

```bash
git clone https://github.com/NVIDIA-AI-IOT/torch2trt
